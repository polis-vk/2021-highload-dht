# Отчет 
### Этап 4. Шардирование

---

В данной работе было реализовано горизонтальное масштабирование через поддержку кластерных конфигураций, состоящих из 
нескольких узлов, взаимодействующих друг с другом через реализованный HTTP API.
В качестве алгоритма распределения данных между узлами был выбран **consistent hashing**.

Для профилирования и анализа запускалась утилита wrk по 3-м узлам сервиса.
## PUT запросы
Параметры запуска:
* `wrk -c 64 -t 4 -R 15000 -s put.lua -d 2m -L http://localhost:8080`
* `wrk -c 64 -t 4 -R 15000 -s put.lua -d 2m -L http://localhost:8081`
* `wrk -c 64 -t 4 -R 15000 -s put.lua -d 2m -L http://localhost:8082`

Результаты:
* [wrk PUT 8080](log/put/put_cpu_8080)
* [wrk PUT 8081](log/put/put_cpu_8081)
* [wrk PUT 8082](log/put/put_cpu_8082)

#### CPU
| 8080 | 8081 | 8082 |
| :---: | :----: | :---: |
| ![put_cpu_8080.png](res/put/put_cpu_8080.png) | ![put_cpu_8081.png](res/put/put_cpu_8081.png) | ![put_cpu_8082.png](res/put/put_cpu_8082.png) |

Из результатов видно, что нагрузка равномерно распределяется между узлами.
Кроме того, появились дополнительные затраты на перенаправление запросов, работу с сетью.
#### Alloc
![put_mem.png](res/put/put_mem.png)
Аналогично CPU появились затраты на формирование перенаправления запроса на другой узел.

#### Loc
![put_lock.png](res/put/put_lock.png)

### GET запросы
Параметры запуска:
* `wrk -c 64 -t 4 -R 15000 -s get.lua -d 2m -L http://localhost:8080`
* `wrk -c 64 -t 4 -R 15000 -s get.lua -d 2m -L http://localhost:8081`
* `wrk -c 64 -t 4 -R 15000 -s get.lua -d 2m -L http://localhost:8082`

Результаты:
* [wrk GET 8080](log/get/get_cpu_8080)
* [wrk GET 8081](log/get/get_cpu_8081)
* [wrk GET 8082](log/get/get_cpu_8082)

#### CPU
| 8080 | 8081 | 8082 |
| :---: | :----: | :---: |
| ![get_cpu_8080.png](res/get/get_cpu_8080.png) | ![get_cpu_8081.png](res/get/get_cpu_8081.png) | ![get_cpu_8082.png](res/get/get_cpu_8082.png) |

Из результатов видно, что нагрузка равномерно распределяется между узлами.
Кроме того, появились дополнительные затраты на перенаправление запросов, работу с сетью.
#### Alloc
![get_mem.png](res/get/get_mem.png)
Аналогично CPU появились затраты на формирование перенаправления запроса на другой узел.

#### Loc
![get_lock.png](res/get/get_lock.png)

---
### Сравнение с предыдущей версией
#### GET
|                  Async                   |                   Sync                   |
|:----------------------------------------:|:----------------------------------------:|
| ![get_cpu.png](res/get/get_cpu_old.png)  | ![get_cpu.png](res/get/get_cpu_8080.png) |
| ![get_cpu.png](res/get/get_mem_old.png)  |   ![get_cpu.png](res/get/get_mem.png)    |
| ![get_cpu.png](res/get/get_lock_old.png) |   ![get_cpu.png](res/get/get_lock.png)   |

#### PUT
|                  Async                   |                   Sync                   |
|:----------------------------------------:|:----------------------------------------:|
| ![get_cpu.png](res/put/put_cpu_old.png)  | ![get_cpu.png](res/put/put_cpu_8080.png) |
| ![get_cpu.png](res/put/put_mem_old.png)  |   ![get_cpu.png](res/put/put_mem.png)    |
| ![get_cpu.png](res/put/put_lock_old.png) |   ![get_cpu.png](res/put/put_lock.png)   |

---
Можно заметить, что в сравнении с версией из 3-го этапа (без шардирования) при одинаковой загрузке
среднее время обработки запросов сильно не изменилось, но в новой версии немного увеличелось максимальное время 
обработки и дисперсия. Данное изменение связано с ситуацией, когда запрос перенаправляется на другие узлы.

Варианты оптимизации - подбирать значение количества виртуальных нод в **consistent hashing** алгоритме, подбирать 
хэш функцию. В случае с хостами разной мощности следует адаптивно выделять промежутки попадания хэш функции и 
количество виртуальных нод, чтобы более мощному перенаправлялось больше запросов. Кроме того, можно заметить учеличение 
работы сборщика мусора, а значит это тоже можно попробовать оптимизировать.